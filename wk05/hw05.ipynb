{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Spark version\n",
    "Install Spark and PySpark\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructField, StructType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spark Version : 3.5.1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SPARK SESSION\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())\n",
    "\n",
    "display(f\"Spark Version : {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: FHV October 2019\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMA\n",
    "fhv_schema = StructType([\n",
    "    StructField(\"dispatching_base_num\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropOff_datetime\", TimestampType(), True),\n",
    "    StructField(\"PUlocationID\", IntegerType(), True),\n",
    "    StructField(\"DOlocationID\", IntegerType(), True),\n",
    "    StructField(\"SR_Flag\", DoubleType(), True),\n",
    "    StructField(\"Affiliated_base_number\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average file size of partitioned parquet is: 6 MB'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = \"./data/raw/fhv/fhv_tripdata_2019-10.csv.gz\"\n",
    "pq_dir = \"./data/pq/fhv/\"\n",
    "\n",
    "# READ CSV\n",
    "fhv_df = (spark.read\n",
    "          .option(\"header\", \"true\")\n",
    "          .schema(fhv_schema)\n",
    "          .csv(csv_path))\n",
    "\n",
    "# REPARTITION TO PARQUET\n",
    "fhv_df.repartition(6).write.parquet(pq_dir, mode=\"overwrite\")\n",
    "\n",
    "# AVERAGE SIZE OF THE PARQUET FILES\n",
    "pq_file_sizes = [os.path.getsize(os.path.join(pq_dir, f)) for f in os.listdir(\"./data/pq/fhv/\") if f[-8:] == \".parquet\"]\n",
    "average_pq_size = round(sum(pq_file_sizes) / len(pq_file_sizes) / 1024 / 1024)\n",
    "\n",
    "display(f\"Average file size of partitioned parquet is: {average_pq_size} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Count records\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of trips on 2019-10-15: 62,610'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE TEMP VIEW FOR QUERY\n",
    "fhv_df.createOrReplaceTempView(\"fhv_data\")\n",
    "\n",
    "## Using spark.sql\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT count(1) as trips_count FROM fhv_data WHERE CAST(pickup_datetime AS DATE) = '2019-10-15';\n",
    "# \"\"\").show()\n",
    "\n",
    "## Using spark.filter\n",
    "number_of_trips = fhv_df.filter(F.to_date(\"pickup_datetime\") == '2019-10-15').count()\n",
    "\n",
    "display(f\"Number of trips on 2019-10-15: {number_of_trips:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Longest trip for each day\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The longest trip in the dataset took : 631,152.5 Hrs'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_duration = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    MAX(UNIX_TIMESTAMP(dropOff_datetime) - UNIX_TIMESTAMP(pickup_datetime)) / 3600 AS trip_duration\n",
    "FROM fhv_data;\n",
    "\"\"\").collect()\n",
    "\n",
    "display(f\"The longest trip in the dataset took : {max_duration[0]['trip_duration']:,.1f} Hrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: User Interface\n",
    "\n",
    "Sparkâ€™s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "[Link to Spark dashboard / localhost:4040](http://localhost:4040)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Least frequent pickup location zone\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE `zone_data` TEMP VIEW\n",
    "(spark.read\n",
    " .option(\"header\", \"true\")\n",
    " .schema(StructType([\n",
    "    StructField(\"LocationID\", IntegerType(), True),\n",
    "    StructField(\"Borough\", StringType(), True),\n",
    "    StructField(\"Zone\", StringType(), True),\n",
    "    StructField(\"service_zone\", StringType(), True),]))\n",
    " .csv(\"./zones/taxi_zone_lookup.csv\")\n",
    " .createOrReplaceTempView(\"zone_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The least picked up zone in the dataset is : Jamaica Bay'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# JOIN `fhv_data` and `zone_data`\n",
    "least_picked_up_zone = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pul.Zone as pickup_zone,\n",
    "    COUNT(1) as picked_up_count\n",
    "FROM \n",
    "    fhv_data\n",
    "        LEFT JOIN zone_data pul ON fhv_data.PUlocationID = pul.LocationID\n",
    "        LEFT JOIN zone_data dol ON fhv_data.DOlocationID = dol.LocationID\n",
    "GROUP BY \n",
    "    pickup_zone\n",
    "ORDER BY\n",
    "    picked_up_count\n",
    "LIMIT 1;\n",
    "\"\"\").collect()\n",
    "\n",
    "display(f\"The least picked up zone in the dataset is : {least_picked_up_zone[0]['pickup_zone']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
