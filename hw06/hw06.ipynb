{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Redpanda version\n",
    "\n",
    "```bash\n",
    "$ rpk version\n",
    "v22.3.5 (rev 28b2443)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Creating a topic\n",
    "\n",
    "```bash\n",
    "$ rpk topic create test-topic\n",
    "TOPIC       STATUS\n",
    "test-topic  OK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Connecting to the Kafka server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Sending data to the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "Sent: {'number': 10}\n",
      "Sent: {'number': 11}\n",
      "Sent: {'number': 12}\n",
      "Sent: {'number': 13}\n",
      "Sent: {'number': 14}\n",
      "Sent: {'number': 15}\n",
      "Sent: {'number': 16}\n",
      "Sent: {'number': 17}\n",
      "Sent: {'number': 18}\n",
      "Sent: {'number': 19}\n",
      "Sent: {'number': 20}\n",
      "Sent: {'number': 21}\n",
      "Sent: {'number': 22}\n",
      "Sent: {'number': 23}\n",
      "Sent: {'number': 24}\n",
      "Sent: {'number': 25}\n",
      "Sent: {'number': 26}\n",
      "Sent: {'number': 27}\n",
      "Sent: {'number': 28}\n",
      "Sent: {'number': 29}\n",
      "Sent: {'number': 30}\n",
      "Sent: {'number': 31}\n",
      "Sent: {'number': 32}\n",
      "Sent: {'number': 33}\n",
      "Sent: {'number': 34}\n",
      "Sent: {'number': 35}\n",
      "Sent: {'number': 36}\n",
      "Sent: {'number': 37}\n",
      "Sent: {'number': 38}\n",
      "Sent: {'number': 39}\n",
      "Sent: {'number': 40}\n",
      "Sent: {'number': 41}\n",
      "Sent: {'number': 42}\n",
      "Sent: {'number': 43}\n",
      "Sent: {'number': 44}\n",
      "Sent: {'number': 45}\n",
      "Sent: {'number': 46}\n",
      "Sent: {'number': 47}\n",
      "Sent: {'number': 48}\n",
      "Sent: {'number': 49}\n",
      "Sent: {'number': 50}\n",
      "Sent: {'number': 51}\n",
      "Sent: {'number': 52}\n",
      "Sent: {'number': 53}\n",
      "Sent: {'number': 54}\n",
      "Sent: {'number': 55}\n",
      "Sent: {'number': 56}\n",
      "Sent: {'number': 57}\n",
      "Sent: {'number': 58}\n",
      "Sent: {'number': 59}\n",
      "Sent: {'number': 60}\n",
      "Sent: {'number': 61}\n",
      "Sent: {'number': 62}\n",
      "Sent: {'number': 63}\n",
      "Sent: {'number': 64}\n",
      "Sent: {'number': 65}\n",
      "Sent: {'number': 66}\n",
      "Sent: {'number': 67}\n",
      "Sent: {'number': 68}\n",
      "Sent: {'number': 69}\n",
      "Sent: {'number': 70}\n",
      "Sent: {'number': 71}\n",
      "Sent: {'number': 72}\n",
      "Sent: {'number': 73}\n",
      "Sent: {'number': 74}\n",
      "Sent: {'number': 75}\n",
      "Sent: {'number': 76}\n",
      "Sent: {'number': 77}\n",
      "Sent: {'number': 78}\n",
      "Sent: {'number': 79}\n",
      "Sent: {'number': 80}\n",
      "Sent: {'number': 81}\n",
      "Sent: {'number': 82}\n",
      "Sent: {'number': 83}\n",
      "Sent: {'number': 84}\n",
      "Sent: {'number': 85}\n",
      "Sent: {'number': 86}\n",
      "Sent: {'number': 87}\n",
      "Sent: {'number': 88}\n",
      "Sent: {'number': 89}\n",
      "Sent: {'number': 90}\n",
      "Sent: {'number': 91}\n",
      "Sent: {'number': 92}\n",
      "Sent: {'number': 93}\n",
      "Sent: {'number': 94}\n",
      "Sent: {'number': 95}\n",
      "Sent: {'number': 96}\n",
      "Sent: {'number': 97}\n",
      "Sent: {'number': 98}\n",
      "Sent: {'number': 99}\n",
      "took 5.08 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "t0 = time.time()\n",
    "measured_time_dict = {\"send\": [], \"flush\": []}\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(100):\n",
    "    message = {'number': i}\n",
    "    start = timeit.default_timer()\n",
    "    producer.send(topic_name, value=message)\n",
    "    measured_time_dict[\"send\"].append(timeit.default_timer() - start)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "producer.flush()\n",
    "measured_time_dict[\"flush\"].append(timeit.default_timer() - start)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004262909997487441"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0002246999938506633"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'When tested over 100 runs, send and flush almost took save time. with ~.1ms difference'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sum(measured_time_dict[\"send\"]) / len(measured_time_dict[\"send\"]))\n",
    "display(sum(measured_time_dict[\"flush\"]) / len(measured_time_dict[\"flush\"]))\n",
    "\n",
    "display(\"When tested over 100 runs, send and flush almost took save time. with ~.1ms difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Sending the Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.79525179998018\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "df_green = pd.read_csv(\"green_tripdata_2019-10.csv.gz\", low_memory=False)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    message = row_dict\n",
    "    producer.send(\"green-trips\", value=message)\n",
    "\n",
    "print(timeit.default_timer() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(query.isActive)\n",
    "query.stop()\n",
    "display(query.isActive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")\n",
    "\n",
    "# query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_stream = green_stream.withColumn(\"timestamp\", F.current_timestamp())\n",
    "\n",
    "popular_destinations = green_stream.groupBy(\n",
    "    F.window(F.col(\"timestamp\"), \"5 minutes\"),\n",
    "    F.col(\"DOLocationID\")\n",
    ").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"res_table\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|count|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 18:30...|          74|35482|\n",
      "|{2024-04-09 18:30...|          42|31884|\n",
      "|{2024-04-09 18:30...|          41|28122|\n",
      "|{2024-04-09 18:30...|          75|25680|\n",
      "|{2024-04-09 18:30...|         129|23860|\n",
      "|{2024-04-09 18:30...|           7|23066|\n",
      "|{2024-04-09 18:30...|         166|21690|\n",
      "|{2024-04-09 18:30...|         236|15826|\n",
      "|{2024-04-09 18:30...|         223|15084|\n",
      "|{2024-04-09 18:30...|         238|14636|\n",
      "|{2024-04-09 18:30...|          82|14584|\n",
      "|{2024-04-09 18:30...|         181|14564|\n",
      "|{2024-04-09 18:30...|          95|14488|\n",
      "|{2024-04-09 18:30...|         244|13466|\n",
      "|{2024-04-09 18:30...|          61|13212|\n",
      "|{2024-04-09 18:30...|         116|12678|\n",
      "|{2024-04-09 18:30...|         138|12288|\n",
      "|{2024-04-09 18:30...|          97|12100|\n",
      "|{2024-04-09 18:30...|          49|10442|\n",
      "|{2024-04-09 18:30...|         151|10306|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM res_table ORDER BY count DESC\").show()\n",
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
